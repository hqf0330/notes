{"daily/202501w2":{"title":"202501w2 report","links":[],"tags":["daily"],"content":"what I have done!\n\n新用户承接的报表和数据迁移\n出问题啦，12.31后的数据不更新了，或者200000平台的埋点没上报\n人群圈选数据梳理，指标口径文档梳理\n\n1.8 日报 冰湖\n昨日进度：\n\n第一期的存算基座搭建和部署方案输出\n技术选型并且在自己的服务器上进行部署测试\n今日计划：\n测试组件\n实时找数据\n\n需求，增长看盘数据排查\n新用户承接对有问题对地方进行兼容\nds-redash组件编译部署和文档编写\n凯叔大数据核心文档\n梳理表的对应关系，拆解一下关键字和派生指标\n1.10 日报 冰湖\n昨日进度：\n\n大数据基座选型完成，技术文档输出\nredash看板使用手册输出\n组件本地适配成功\n\n今日计划：\n\n编写看板培训手册\n集群基础运维手册\n"},"daily/202501w3":{"title":"202501w3","links":[],"tags":[],"content":"Highlight\n\n 日志接收服务，FastApi版本开发\n\nTODO\n\n 东升基础环境搭建\n 实时标签——方案梳理\n 整体的飞书告警功能\n\nDONE\n\n 新用户承接module_id修改确认\n 对数据，尤其三方账单部分\n 用户标签开发\n 人群包上传太大\n 东升的数据需求拆解分析文档输出\n horus eye的数据交换建设完毕\n"},"daily/202502w1":{"title":"2025重点需要完成事项","links":[],"tags":["daily"],"content":"TODO\n\n 凯叔实时标签梳理\n 凯叔荷鲁斯之眼专项\n\n梳理拓展业务指标，目的是直观发现出业务上的问题\n\n\n 凯叔软著材料收尾\n 凯叔运营中心后台数据交换搭建\n 东升日志接收服务和埋点验收\n 东升报表建设与开发\n"},"daily/test":{"title":"test","links":[],"tags":[],"content":"\nhahha"},"dataware/bitmap":{"title":"bitmap--the killer of User Profile System","links":[],"tags":["dataware","bitmap"],"content":"什么是bitmap\nBit-Map算法又名位图算法，其原理是，使用下标代替数值或特定的意义，使用这个位为0或者1代表特性是否存在\nBit-Map算法具有效率高，节省空间的特点，适用于对大量数据进行去重，查询等\n\n\n                  \n                  NOTE\n                  \n                \n\n对于某个标签，给出一个长度为n的数组，初始化都为0，如果来了一个用户或者一个用户符合当\n前标签，我们就将这个数组的某个位置置为1。基于当前标签进行统计某些值的时候就很快\n\n\n从案例来理解bitmap\n\n建立用户名和id的映射关系\n现在有标签宽表如下：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNameSexAgeOccupationPhoneAmale90sDeviPhoneBmale90sdevSanCfemale00sstudentmi\n构建映射关系之后\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIDName1A2B3C\n\n将标签宽表使用bitmap进行拆分\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSexbitmapmale1, 2female3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgebitmap90s1, 200s3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOccupationbitmapDev1, 2student3\n\n使用bitmap进行查询\n\nbitma的使用\nbitmap的存储\n如果用户id是int类型，那么就是有4 bytes, 32bit的存储开销，而在bitmap中，一个用户实际上也就只占来一个bit，内存开销直接节省32倍\nbitmap的使用\n\n\n                  \n                  TIP\n                  \n                \n\nbitmap在用户群做交集和并集运算的时候速度很快，主要是用位图异或计算\n\n\n我想计算是dev，并且使用iPhone的人群\n\n我想计算male 或 00s的人群\n\n由此可以发现，使用位进行计算的速度是非常快，计算的效率非常高\n我想计算 not 90s的人群\n从age的表中可以知道，90s只有AB两人，00s则有C一人。not 90s人群，也就是计算出C来。如果直接计算not 90s，这样就会导致一个问题，具体结果如下：\n\n\n                  \n                  IMPORTANT\n                  \n                \n\n直接计算不太可行，因此还要借助一个全量用户来进行辅助计算，90s XOR ALL = not 90s，也就是异或计算：相同为0，不同为1\n\n\nbitmap的优化"},"dataware/user_profile_system":{"title":"Let's us to talk about User Profile System","links":[],"tags":["dataware"],"content":"What is the User Profile System"},"docker/1_docker_network":{"title":"docker network","links":[],"tags":["docker","network"],"content":"容器通信\n当项目大规模使用 Docker 时，容器通信的问题也就产生了。\n要解决容器通信问题，必须先了解很多关于网络的知识。\n我们需要了解Docker 的网络知识，以满足更高的网络需求。\n1. 默认网络\n装 Docker 以后，会默认创建三种网络，可以通过 docker network ls 查看\n[root@localhost ~]# docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nf869d1c3534a        bridge              bridge              local\n1543d4d4b945        host                host                local\nd06a4fca4238        none                null                local\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n网络模式简介bridge为每一个容器分配、设置 IP 等，并将容器连接到一个 docker0 虚拟网桥，默认为该模式。host容器将不会虚拟出自己的网卡，配置自己的 IP 等，而是使用宿主机的 IP 和端口。none容器有独立的 Network namespace，但并没有对其进行任何网络设置，如分配 veth pair 和网桥连接，IP 等。container新创建的容器不会创建自己的网卡和配置自己的 IP，而是和一个指定的容器共享 IP、端口范围等。\n1.1 bridge 网络模式\n在该模式中，Docker 守护进程创建了一个虚拟以太网桥 docker0，新建的容器会自动桥接到这个接口，\n附加在其上的任何网卡之间都能自动转发数据包。\n默认情况下，守护进程会创建一对对等虚拟设备接口 veth pair，将其中一个接口设置为容器的 eth0\n接口（容器的网卡），另一个接口放置在宿主机的命名空间中，以类似 vethxxx 这样的名字命名，\n从而将宿主机上的所有容器都连接到这个内部网络上。\nveth是linux的一种虚拟网络设备，它有点类似于两张网卡中间用一条网线连着，veth设备总是成对出现，\n通常用来连 不同网络命名空间（下面开始简称NS），一端连着NS1的内核协议栈，另一端连着NS2的内核协议栈\n，一端发送的数据会立刻被另一端接收。\n\n比如我运行一个基于 busybox 镜像构建的容器 bbox01，查看 ip addr：\n\nbusybox 被称为嵌入式 Linux 的瑞士军刀，整合了很多小的 unix 下的通用功能到一个小的可执行文件中\n\n\n然后宿主机通过 ip addr 查看信息如下：\n\n通过以上的比较可以发现，证实了之前所说的：守护进程会创建一对对等虚拟设备接口 veth pair，将其中一个接口设置为容器的 eth0 接口（容器的网卡），另一个接口放置在宿主机的命名空间中，以类似 vethxxx 这样的名字命名。\n　　同时，守护进程还会从网桥 docker0 的私有地址空间中分配一个 IP 地址和子网给该容器，并设置 docker0 的 IP 地址为容器的默认网关。也可以安装 yum install -y bridge-utils 以后，通过 brctl show 命令查看网桥信息。\n\n对于每个容器的 IP 地址和 Gateway 信息，我们可以通过 docker inspect 容器名称|ID 进行查看，在 NetworkSettings 节点中可以看到详细信息。\n\n我们可以通过 docker network inspect bridge 查看所有 bridge 网络模式下的容器，在 Containers 节点中可以看到容器名称。\n\n\n　关于 bridge 网络模式的使用，只需要在创建容器时通过参数 --net bridge 或者 --network bridge 指定即可，当然这也是创建容器默认使用的网络模式，也就是说这个参数是可以省略的。\n\n\nBridge 桥接模式的实现步骤主要如下：\n\nDocker Daemon 利用 veth pair 技术，在宿主机上创建一对对等虚拟网络接口设备，假设为 veth0 和 veth1。而\nveth pair 技术的特性可以保证无论哪一个 veth 接收到网络报文，都会将报文传输给另一方。\nDocker Daemon 将 veth0 附加到 Docker Daemon 创建的 docker0 网桥上。保证宿主机的网络报文可以发往 veth0；\nDocker Daemon 将 veth1 添加到 Docker Container 所属的 namespace 下，并被改名为 eth0。如此一来，宿主机的网络报文若发往 veth0，则立即会被 Container 的 eth0 接收，实现宿主机到 Docker Container 网络的联通性；同时，也保证 Docker Container 单独使用 eth0，实现容器网络环境的隔离性。\n\n1.2 host网络模式\n\nhost 网络模式需要在创建容器时通过参数 --net host 或者 --network host 指定；\n采用 host 网络模式的 Docker Container，可以直接使用宿主机的 IP 地址与外界进行通信，若宿主机的 eth0 是一个公有 IP，那么容器也拥有这个公有 IP。同时容器内服务的端口也可以使用宿主机的端口，无需额外进行 NAT 转换；\nhost 网络模式可以让容器共享宿主机网络栈，这样的好处是外部主机与容器直接通信，但是容器的网络缺少隔离性。\n\n\n比如我基于 host 网络模式创建了一个基于 busybox 镜像构建的容器 bbox02，查看 ip addr：\n\n然后宿主机通过 ip addr 查看信息如下：\n\n返回信息一模一样，我们可以通过 docker network inspect host 查看所有 host 网络模式下的容器，在 Containers 节点中可以看到容器名称。\n\n1.3 none网络模式\n\nnone 网络模式是指禁用网络功能，只有 lo 接口( local 的简写)，代表 127.0.0.1，即 localhost 本地环回接口。在创建容器时通过参数 --net none 或者 --network none 指定；\nnone 网络模式即不为 Docker Container 创建任何的网络环境，容器内部就只能使用 loopback 网络设备，不会再有其他的网络资源。可以说 none 模式为 Docke Container 做了极少的网络设定，但是俗话说得好“少即是多”，在没有网络配置的情况下，作为 Docker 开发者，才能在这基础做其他无限多可能的网络定制开发。这也恰巧体现了 Docker 设计理念的开放。\n\n比如我基于 none 网络模式创建了一个基于 busybox 镜像构建的容器 bbox03，查看 ip addr：\n\n我们可以通过 docker network inspect none 查看所有 none 网络模式下的容器，在 Containers 节点中可以看到容器名称。\n\n1.4 container网络模式\n\nContainer 网络模式是 Docker 中一种较为特别的网络的模式。在创建容器时通过参数 --net container:已运行的容器名称|ID 或者 --network container:已运行的容器名称|ID 指定；\n处于这个模式下的 Docker 容器会共享一个网络栈，这样两个容器之间可以使用 localhost 高效快速通信。\n\n\nContainer 网络模式即新创建的容器不会创建自己的网卡，配置自己的 IP，而是和一个指定的容器共享 IP、端口范围等。同样两个容器除了网络方面相同之外，其他的如文件系统、进程列表等还是隔离的。\n　　\n　　比如我基于容器 bbox01 创建了 container 网络模式的容器 bbox04，查看 ip addr：\n\n\n宿主机的 ip addr 信息如下：\n\n通过以上测试可以发现，Docker 守护进程只创建了一对对等虚拟设备接口用于连接 bbox01 容器和宿主机，而 bbox04 容器则直接使用了 bbox01 容器的网卡信息。\n这个时候如果将 bbox01 容器停止，会发现 bbox04 容器就只剩下 lo 接口了。\n\n然后 bbox01 容器重启以后，bbox04 容器也重启一下，就又可以获取到网卡信息了。\n\n2. link\ndocker run --link 可以用来链接两个容器，使得源容器（被链接的容器）和接收容器（主动去链接的容器）之间可以互相通信，并且接收容器可以获取源容器的一些数据，如源容器的环境变量。\n　　这种方式官方已不推荐使用，并且在未来版本可能会被移除，所以这里不作为重点讲解，感兴趣可自行了解。\n　　官网警告信息：docs.docker.com/network/links/\n3. 自定义网络\n虽然 Docker 提供的默认网络使用比较简单，但是为了保证各容器中应用的安全性，在实际开发中更推荐使用自定义的网络进行容器管理，以及启用容器名称到 IP 地址的自动 DNS 解析。\n\n　　从 Docker 1.10 版本开始，docker daemon 实现了一个内嵌的 DNS server，使容器可以直接通过容器名称通信。方法很简单，只要在创建容器时使用 --name 为容器命名即可。\n　　但是使用 Docker DNS 有个限制：只能在 user-defined 网络中使用。也就是说，默认的 bridge 网络是无法使用 DNS 的，所以我们就需要自定义网络。\n\n3.1 创建网络\n通过 docker network create 命令可以创建自定义网络模式，命令提示如下：\n\n进一步查看 docker network create 命令使用详情，发现可以通过 --driver 指定网络模式且默认是 bridge 网络模式，提示如下：\n\n创建一个基于 bridge 网络模式的自定义网络模式 custom_network，完整命令如下：\ndocker network create custom_network\n通过 docker network ls 查看网络模式：\n[root@localhost ~]# docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nb3634bbd8943        bridge              bridge              local\n062082493d3a        custom_network      bridge              local\n885da101da7d        host                host                local\nf4f1b3cf1b7f        none                null                local\n通过自定义网络模式 custom_network 创建容器：\ndocker run -di --name bbox05 --net custom_network busybox\n通过 docker inspect 容器名称|ID 查看容器的网络信息，在 NetworkSettings 节点中可以看到详细信息。\n\n3.2 连接网络\n通过 docker network connect 网络名称 容器名称 为容器连接新的网络模式。\n\ndocker network connect bridge bbox05\n　通过 docker inspect 容器名称|ID 再次查看容器的网络信息，多增加了默认的 bridge。\n\n3.3 断开网络\n通过 docker network disconnect 网络名称 容器名称 命令断开网络。\ndocker network disconnect custom_network bbox05\n通过 docker inspect 容器名称|ID 再次查看容器的网络信息，发现只剩下默认的 bridge。\n\n3.4 移除网络\n可以通过 docker network rm 网络名称 命令移除自定义网络模式，网络模式移除成功会返回网络模式名称。\ndocker network rm custom_network\n注意：如果通过某个自定义网络模式创建了容器，则该网络模式无法删除。\n4. 容器间网络通信\n首先明确一点，容器之间要互相通信，必须要有属于同一个网络的网卡。\n　　我们先创建两个基于默认的 bridge 网络模式的容器。\ndocker run -di --name default_bbox01 busybox\ndocker run -di --name default_bbox02 busybox\n通过 docker network inspect bridge 查看两容器的具体 IP 信息。\n\n然后测试两容器间是否可以进行网络通信。\n\n经过测试，从结果得知两个属于同一个网络的容器是可以进行网络通信的，但是 IP 地址可能是不固定的，有被更改的情况发生，那容器内所有通信的 IP 地址也需要进行更改，能否使用容器名称进行网络通信？继续测试。\n\n经过测试，从结果得知使用容器进行网络通信是不行的，那怎么实现这个功能呢？\n　　从 Docker 1.10 版本开始，docker daemon 实现了一个内嵌的 DNS server，使容器可以直接通过容器名称通信。方法很简单，只要在创建容器时使用 --name 为容器命名即可。\n　　但是使用 Docker DNS 有个限制：只能在 user-defined 网络中使用。也就是说，默认的 bridge 网络是无法使用 DNS 的，所以我们就需要自定义网络。\n　　我们先基于 bridge 网络模式创建自定义网络 custom_network，然后创建两个基于自定义网络模式的容器。\ndocker network create custom_network\n \ndocker run -di --name custom_bbox01 --net custom_network busybox\ndocker run -di --name custom_bbox02 --net custom_network busybox\n通过 docker network inspect custom_network 查看两容器的具体 IP 信息\n\n然后测试两容器间是否可以进行网络通信，分别使用具体 IP 和容器名称进行网络通信。\n"},"doris/0_om_doris":{"title":"Doris的常见问题与解决思路","links":["doris/1_prepare_for_daris_manager"],"tags":["doris","om","man"],"content":"Doris Manager的安装部署\n首先根据下面的内容安装和部署Doris Manager\n1_prepare_for_daris_manager.md"},"doris/1_prepare_for_daris_manager":{"title":"Installation and Deployment Doris Manager","links":[],"tags":["doris","dataware"],"content":"下载Doris Manager"},"index":{"title":"Welcome to BingHu House","links":["doris/0_om_doris","doris/1_prepare_for_daris_manager","docker/1_docker_network","k8s/1_k8s_基础架构","k8s/2_k8s_网络"],"tags":[],"content":"Doris\n0_om_doris.md\n1_prepare_for_daris_manager.md\nDocker\n1_docker_network.md\nK8s\n1_k8s_基础架构.md\n2_k8s_网络.md"},"k8s/1_k8s_基础架构":{"title":"k8s的架构和概念","links":[],"tags":["k8s","architecture"],"content":"应用部署方式的演变\n在部署应用程序的方式上，主要经历了三个时代:\n\n\n传统部署: 互联网早期，会将应用程序直接部署在物理机上\n\n优点：简单，不需要其他技术参与\n缺点：不能为应用程序定义资源使用边界，很难合理地分配计算资源，而且程序之间容易产生影响\n\n\n\n虚拟化部署: 可以在一台物理机上运行多个虚拟机，每个虚拟机都是独立的一个环境\n\n优点：程序环境不会相互产生影响，提供了一定程度的安全性\n缺点：增加了操作系统，浪费了部分资源\n\n\n\n容器部署: 与虚拟化类似，但是共享了操作系统\n\n优点：可以保证每个容器拥有自己的文件系统、CPU、内存、进程空间等\n运行应用程序所需要的资源都被容器包装，并和底层基础架构解耦\n容器化的应用程序可以跨云服务商、跨Linux操作系统发行版进行部署\n\n\n\n\n容器化部署方式给带来很多的便利，但是也会出现一些问题，比如说：\n\n\n                  \n                  QUESTION\n                  \n                \n\n\n一个容器故障停机了，怎么样让另外一个容器立刻启动去替补停机的容器\n当并发访问量变大的时候，怎么样做到横向扩展容器数量\n\n\n\n这些容器管理的问题统称为容器编排问题，为了解决这些容器编排问题，就产生了一些容器编排的软件：\nSwarm: Docker自己的容器编排工具\nMesos: Apache的一个资源统一管控的工具，需要和Marathon结合使用\nKubernetes: Google开源的的容器编排工具\nkubernetes简介\nkubernetes，是一个全新的基于容器技术的分布式架构领先方案，是谷歌严格保密十几年的秘密武器Borg系统\n的一个开源版本，于2014年9月发布第一个版本，2015年7月发布第一个正式版本。kubernetes的本质是一组服\n务器集群，它可以在集群的每个节点上运行特定的程序，来对节点中的容器进行管理。目的是实现资源管理的\n自动化，主要提供了如下的主要功能:\n\n自我修复: 一旦某一个容器崩溃，能够在1秒中左右迅速启动新的容器\n弹性伸缩: 可以根据需要，自动对集群中正在运行的容器数量进行调整\n服务发现: 服务可以通过自动发现的形式找到它所依赖的服务\n负载均衡: 如果一个服务起动了多个容器，能够自动实现请求的负载均衡\n版本回退: 如果发现新发布的程序版本有问题，可以立即回退到原来的版本\n存储编排: 可以根据容器自身的需求自动创建存储卷\n\nkubernetes组件\n一个kubernetes集群主要是由控制节点(master),**工作节点(node)**构成，每个节点上都会安装不同的组件\n\n\nmaster: 集群的控制平面，负责集群的决策(管理)\n\nApiServer: 资源操作的唯一入口，接收用户输入的命令，提供认证、授权、API注册和发现等机制\nScheduler: 负责集群资源调度，按照预定的调度策略将Pod调度到相应的node节点上\nControllerManager: 负责维护集群的状态，比如程序部署安排、故障检测、自动扩展、滚动更新等\nEtcd: 负责存储集群中各种资源对象的信息\n\n\n\nnode: 集群的数据平面，负责为容器提供运行环境(干活)\n\nKubelet: 负责维护容器的生命周期，即通过控制docker，来创建、更新、销毁容器\nKubeProxy: 负责提供集群内部的服务发现和负载均衡\nDocker: 负责节点上容器的各种操作\n\n\n\nkubernetes概念\n\nMaster: 集群控制节点，每个集群需要至少一个master节点负责集群的管控\nNode: 工作负载节点，由master分配容器到这些node工作节点上，然后node节点上的docker负责容器的运行\nController: 控制器，通过它来实现对pod的管理，比如启动pod、停止pod、伸缩pod的数量等等\nService: pod对外服务的统一入口，下面可以维护者同一类的多个pod\nLabel: 标签，用于对pod进行分类，同一类pod会拥有相同的标签\nNameSpace: 命名空间，用来隔离pod的运行环境\n"},"k8s/2_k8s_网络":{"title":"k8s的网络","links":[],"tags":["k8s","network"],"content":"k8s的网络基本概述"},"mysql/1_mysql_deploy":{"title":"MySQL的部署安装","links":[],"tags":["mysql","DBA","depoly"],"content":"MySQL体系架构\nMySQL存储引擎\nLinux安装MySQL"}}